import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math, copy, time
from torch.autograd import Variable


class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super(Embeddings, self).__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model

    def forward(self, x):
        return self.lut(x) * math.sqrt(self.d_model)  # nn.Embedding åˆå§‹åŒ–çš„å‘é‡ æ•°å€¼å¾ˆå°ï¼ˆå¤§å¤šåœ¨ -0.1 ~ 0.1ï¼‰,è€Œ ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ çš„æ•°å€¼å¤§æ¦‚åœ¨ -1 ~ 1 ä¹‹é—´
    # embedding ä¼šè¢«â€œä½ç½®ç¼–ç â€æ·¹æ²¡æ‰ï¼Œæ¨¡å‹å‰æœŸè®­ç»ƒä¼šå¾ˆå›°éš¾,ä¹˜ä»¥ sqrt(d_model) æ˜¯ä¸ºäº†ç»Ÿä¸€æ•°å€¼å°ºåº¦
    # ä½†æ˜¯ä¸ºä»€ä¹ˆæ˜¯æ ¹å·d_model,è¦æ¶‰åŠåˆ°å¤šå¤´æ³¨æ„åŠ›é‡Œçš„ç»†èŠ‚ã€‚æ³¨æ„åŠ›å±‚æŠŠæ•°å€¼é™¤ä»¥äº† sqrt(d_k),Embedding è¿™é‡Œä¹˜ä¸Š sqrt(d_model)ï¼Œä¿æŒä¸åŒæ¨¡å—ä¹‹é—´ç‰¹å¾å€¼çš„ç»Ÿè®¡ä¸€è‡´æ€§
    

# ä½ç½®ç¼–ç ä¸‹æ–¹ä¼šæ ¹æ®ä½ç½®æ·»åŠ ä¸€ä¸ªæ­£å¼¦æ³¢ã€‚æ¯ä¸ªç»´åº¦çš„æ­£å¼¦æ³¢é¢‘ç‡å’Œåç§»é‡éƒ½ä¸åŒ
# å¯¹è¾“å…¥åºåˆ— xï¼ˆå½¢çŠ¶é€šå¸¸æ˜¯ [batch, seq_len, d_model]ï¼‰åŠ ä¸Šå›ºå®šçš„ã€ä¸ä½ç½®ç›¸å…³çš„å‘é‡ pe(pos)ï¼Œå†åš dropoutï¼Œè¿”å›åŠ äº†ä½ç½®ç¼–ç çš„è¡¨ç¤ºã€‚è¿™æ · Transformer å³ä¾¿æ²¡æœ‰ RNN ç»“æ„ï¼Œä¹Ÿèƒ½æ„ŸçŸ¥è¯åºä¿¡æ¯ã€‚è®ºæ–‡ä½¿ç”¨ï¼ˆsin/cosï¼‰å‡½æ•°æ˜¯å› ä¸ºï¼šæ— å‚æ•°ã€èƒ½è¡¨ç¤ºä»»æ„ç›¸å¯¹ä½ç½®ï¼ˆå½¢å¼ä¸Šæ–¹ä¾¿æ¨¡å‹é€šè¿‡çº¿æ€§å˜æ¢æ¨å¯¼å‡ºç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼‰ä¸”å¯æ³›åŒ–åˆ°æ›´é•¿åºåˆ—ã€‚
# seq_lenä»£è¡¨å½“å‰batch çš„æœ€å¤§å¥å­é•¿åº¦ï¼Œè¾“å…¥å¼ é‡åœ¨æ¯ä¸ªbatchå†…æ˜¯ç»Ÿä¸€é•¿åº¦çš„ï¼ˆç»è¿‡ paddingï¼‰ï¼Œä½†ä¸åŒ batch çš„æœ€å¤§é•¿åº¦å¯èƒ½ä¸ä¸€æ ·ï¼Œseq_lenä¹Ÿå°±å¯èƒ½ä¸ä¸€æ ·
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # peå…ˆåˆ›å»ºä¸€ä¸ª (max_len, d_model) çš„é›¶å¼ é‡ï¼Œç”¨æ¥å¡«å…¥ä½ç½®ç¼–ç 
        pe = torch.zeros(max_len, d_model)
        # positionæ˜¯ä¸€ä¸ªåˆ—å‘é‡ [[0],[1],[2],...,[max_len-1]]ï¼Œç”¨äºè®¡ç®—ä¸åŒä½ç½®çš„ç¼–ç ï¼Œunsqueeze(1)åœ¨ç´¢å¼• 1 çš„ä½ç½®æ’å…¥ä¸€ä¸ªé•¿åº¦ä¸º 1 çš„ç»´åº¦ï¼ˆå³æŠŠä¸€ç»´å˜æˆäºŒç»´çš„â€œåˆ—å‘é‡â€ï¼‰,å½¢çŠ¶ç”±(max_len,)å˜ä¸º (max_len, 1)
        position = torch.arange(0, max_len).unsqueeze(1)
        # div_termæ„é€ ä¸€ç³»åˆ—ä¸åŒé¢‘ç‡çš„å› å­ï¼ˆåªé’ˆå¯¹å¶æ•°ç»´åº¦ï¼‰ï¼Œå¯¹åº”è®ºæ–‡ä¸­å…¬å¼é‡Œçš„ 1 / 10000^{2i/d_model} çš„å¯¹æ•°å½¢å¼ã€‚torch.arange(0, d_model, 2) å–å¶æ•°ç´¢å¼•ï¼ˆ0,2,4,...ï¼‰ï¼Œexp(...) ç»™å‡ºé¢‘ç‡ç¼©æ”¾å› å­ã€‚
        # div_term ç”Ÿæˆäº†ä¸€ç»„éšç»´åº¦æŒ‡æ•°è¡°å‡çš„å› å­ï¼Œç”¨äºæŠŠä½ç½® pos ç¼©æ”¾åˆ°ä¸åŒé¢‘ç‡ä¸Šï¼Œé‡‡ç”¨è¿™ä¸ªæ•°å€¼è¿™ä¹ˆè®¡ç®—çš„åŸå› æ˜¯ç”±è®ºæ–‡é‡Œçš„å…¬å¼å†³å®šçš„
        div_term = torch.exp(torch.arange(0, d_model, 2) *
                             -(math.log(10000.0) / d_model))  
        # å¶æ•°ç»´ç”¨ sin(pos * freq)ï¼Œå¥‡æ•°ç»´ç”¨ cos(pos * freq)ã€‚ç»“æœï¼šæ¯ä¸ªä½ç½® pos å¯¹åº”ä¸€ä¸ªé•¿åº¦ä¸º d_model çš„å‘é‡ï¼Œç»´åº¦é—´ä½¿ç”¨ä¸åŒé¢‘ç‡çš„ sin/cosã€‚
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        # pe.unsqueeze(0) â†’ å˜æˆ (1, max_len, d_model)ï¼Œæ–¹ä¾¿åé¢ç›´æ¥å’Œ x å¹¿æ’­ç›¸åŠ ,x æ˜¯ [batch, seq_len, d_model]ã€‚
        pe = pe.unsqueeze(0)
        # register_buffer('pe', pe)ï¼šæŠŠ pe æ³¨å†Œä¸º module çš„ bufferï¼ˆéš state_dict ä¿å­˜/åŠ è½½ï¼Œä½†ä¸æ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œä¸ä¼šåœ¨ä¼˜åŒ–å™¨ä¸­æ›´æ–°ï¼‰ï¼Œä½ç½®ç¼–ç æ˜¯å›ºå®šçš„å¸¸é‡ï¼Œä¸éœ€è¦æ±‚æ¢¯åº¦ã€‚
        self.register_buffer('pe', pe)
        
    def forward(self, x):
        # self.pe[:, :x.size(1)]ï¼šå–å‡ºå‰ seq_len ä¸ªä½ç½®ç¼–ç ï¼Œå½¢çŠ¶ä¸º (1, seq_len, d_model)ï¼Œä¸ x çš„ (batch, seq_len, d_model) å¯ä»¥å¹¿æ’­ç›¸åŠ ã€‚
        # Variable(..., requires_grad=False)ï¼šè¿™æ˜¯æ—§ç‰ˆ PyTorch é£æ ¼ï¼ˆæ—©æœŸæŠŠ Variable å’Œå¼ é‡åŒºåˆ†å¼€ï¼‰ã€‚register_buffer å·²ç»ä¿è¯ pe ä¸ä¼šè¢«è®­ç»ƒï¼Œæ‰€ä»¥ç°ä»£ä»£ç ä¸­ä¸éœ€è¦å†ç”¨ Variableï¼Œç›´æ¥å†™ x = x + self.pe[:, :x.size(1)] å³å¯ã€‚
        x = x + Variable(self.pe[:, :x.size(1)], 
                         requires_grad=False)
        return self.dropout(x)


# attentionè¿”å›ä¸¤ä¸ªä¸œè¥¿contextï¼ˆåŠ æƒåçš„ valueï¼Œè®°ä½œ xï¼Œè¿™æ˜¯ attention çš„ä¸»è¦è¾“å‡ºï¼Œä¼šç”¨äºåç»­æ‹¼æ¥å¤šå¤´å¹¶çº¿æ€§æŠ•å½±ï¼‰ä¸p_attnï¼ˆæ³¨æ„åŠ›æƒé‡çŸ©é˜µï¼Œè¡¨ç¤ºæ¯ä¸ª Query åœ¨å„ä¸ª Key ä¸Šçš„æ³¨æ„åŠ›åˆ†å¸ƒï¼‰
def attention(query, key, value, mask, dropout=None):
    d_k = query.size(-1)
    # è®¡ç®—æ‰“åˆ†ï¼Œè¡¨ç¤ºæ¯ä¸ª Queryï¼ˆtï¼‰å¯¹æ¯ä¸ª Keyï¼ˆsï¼‰çš„ç›¸ä¼¼åº¦
    scores = torch.matmul(query, key.transpose(-2, -1)) \
             / math.sqrt(d_k)  # ç”¨ âˆšd_k æ¥â€œç¼©æ”¾â€ç‚¹ç§¯åˆ†æ•°ï¼Œä¿æŒæ•°å€¼ç¨³å®š(ä¸ºä»€ä¹ˆæ˜¯é™¤ä»¥è¿™ä¸ªæ•°ï¼Œæ˜¯æ•°å­¦æ¨å¯¼å‡ºæ¥çš„),scores.shape = [batch, h, seq_len_q, seq_len_k]ï¼Œæ¯ä¸ª Query å»â€œæ‰“åˆ†â€æ‰€æœ‰ Key
    # å¦‚æœæœ‰ maskï¼ŒæŠŠ mask==0 çš„ä½ç½®è®¾ä¸º -1e9ï¼ˆè¿‘ä¼¼ -infï¼‰ï¼Œä½¿è¿™äº›ä½ç½®åœ¨ softmax åæƒé‡â‰ˆ0
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    # æ¯ä¸ª Query çš„æƒé‡åˆ†å¸ƒï¼ˆå’Œä¸º1ï¼‰ï¼ŒæŠŠâ€œç›¸ä¼¼åº¦åˆ†æ•°â€å˜æˆâ€œæ¦‚ç‡æƒé‡â€ï¼Œæ¯ä¸ª Query å¯¹æ‰€æœ‰ Key çš„æ‰“åˆ†è½¬æ¢ä¸º éè´Ÿä¸”å’Œä¸º 1 çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆæ³¨æ„åŠ›æƒé‡ï¼‰
    p_attn = F.softmax(scores, dim = -1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn  # è®¡ç®—åŠ æƒå’Œcontext,context = torch.matmul(p_attn, value) â†’ context.shape = [batch, heads, seq_len, d_k]ã€‚p_attnå½¢çŠ¶ä¸º[batch, heads, seq_len_q, seq_len_k]
# contextä¸ºå¾—åˆ°æ¯ä¸ªQueryçš„â€œä¸Šä¸‹æ–‡å‘é‡,ç›¸å½“äºä¸ºQueryèšåˆæ¥è‡ªä¸åŒä½ç½®çš„ä¿¡æ¯,è¿™æ ·æ¯ä¸ªè¯éƒ½èƒ½çœ‹åˆ°å…¶ä»–æ‰€æœ‰è¯


# å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ŒæŠŠæ¨¡å‹çš„æ³¨æ„åŠ›â€œåˆ†æˆå¤šä¸ªå¹³è¡Œçš„è§†è§’â€å»æ•æ‰ä¸åŒçš„ç‰¹å¾ï¼Œå¤šå¤´çš„æ„æ€æ˜¯å¯¹tokenå†…éƒ¨åšæ‹†åˆ†ï¼Œå°†ä¸€ä¸ªtokenæ‹†åˆ†æˆå‡ å¤´ï¼Œè€Œä¸æ˜¯æ‹†åˆ†token
class MultiHeadedAttention(nn.Module):
    # h: å¤´çš„æ•°é‡ï¼ˆheadsï¼‰,d_model: æ•´ä¸ªæ¨¡å‹çš„ç»´åº¦ï¼Œæ¯”å¦‚ 512 æˆ– 768,æ¯ä¸ªå¤´çš„ç»´åº¦ï¼šd_k = d_model // h
    def __init__(self, h, d_model, dropout=0.1):
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)  
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    # query, key, value, æ‰€æœ‰è¾“å…¥å½¢çŠ¶å‡ä¸º [batch_size, seq_len, d_model]
    def forward(self, query, key, value, mask):
        # padding maskï¼ˆç”¨äºå±è”½ padï¼‰,(batch, src_len) æˆ– (batch, tgt_len);subsequent maskï¼ˆé˜²çœ‹æœªæ¥ï¼‰,(1, tgt_len, tgt_len) æˆ– (tgt_len, tgt_len);æœ€ç»ˆéƒ½å¹¿æ’­åˆ°scoresçš„[batch, heads, seq_len_q, seq_len_k]
        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)
        nbatches = query.size(0)
        # è¿™å¥ç”¨ 3 ä¸ªçº¿æ€§å±‚åˆ†åˆ«å¯¹ query, key, value åšæŠ•å½±ï¼Œç„¶åæŠŠæ¯ä¸ªæŠ•å½±reshape æˆå¤šå¤´æ ¼å¼å¹¶æŠŠ head ç»´æŒªåˆ°å‰é¢ï¼Œæœ€ç»ˆå¾—åˆ°æ¯ä¸ªå¼ é‡çš„å½¢çŠ¶ä¸ºï¼š[batch, h, seq_len, d_k]
        # å¯¹æ¯å¯¹æ‰§è¡Œ l(x)ï¼ŒæŠŠ [B, L, D] æ˜ å°„åˆ° [B, L, D]ï¼ˆçº¿æ€§å±‚è¾“å‡ºç»´åº¦ä»ä¸º d_modelï¼‰,.view ç”¨æ¥é‡å¡‘å¼ é‡ï¼Œ-1 è¡¨ç¤ºè®© PyTorch è‡ªåŠ¨æ¨æ–­è¯¥ç»´åº¦ï¼ˆåœ¨è¿™é‡Œå°±æ˜¯ seq_lenï¼‰,self.h å’Œ self.d_k æ˜¯å¤šå¤´æ‹†åˆ†åä¸¤ä¸ªç»´åº¦
        # åˆ—è¡¨æ¨å¯¼ä¼šäº§å‡ºä¸€ä¸ªåŒ…å«ä¸‰ä¸ªå¼ é‡çš„åˆ—è¡¨ï¼Œç„¶åç”¨è§£åŒ…èµ‹å€¼ï¼Œquery.shape = key.shape = value.shape = [B, h, seq_len, d_k]
        query, key, value = \
            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)  
             for l, x in zip(self.linears, (query, key, value))]  # zip(...) ä¼šæŠŠå‰ä¸‰ä¸ªçº¿æ€§å±‚åˆ†åˆ«å’Œ query, key, value é…å¯¹,(l0, query), (l1, key), (l2, value)
        
        x, self.attn = attention(query, key, value, mask=mask, 
                                 dropout=self.dropout)
        
        # æŠŠå¤šå¤´çš„è¾“å‡ºå†æ‹¼æ¥å›æ¥ï¼Œå½¢çŠ¶å˜å› [batch, seq_len, d_model]
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.h * self.d_k)
        return self.linears[-1](x)


# å‰é¦ˆç½‘ç»œä¸»è¦ä½œç”¨ï¼šå¯¹æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºç‹¬ç«‹åœ°åšéçº¿æ€§å˜æ¢ï¼ˆæ‰€ä»¥å« position-wiseï¼‰ï¼›æå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼Œè®©ç½‘ç»œèƒ½å­¦ä¹ æ›´å¤æ‚çš„ç‰¹å¾ç»„åˆï¼›è¿™é‡Œçš„éçº¿æ€§æ˜¯é€šè¿‡ ReLU æ¿€æ´»å®ç°çš„
class PositionwiseFeedForward(nn.Module):
    "Implements FFN equation."
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        # çº¿æ€§å±‚ï¼ŒæŠŠè¾“å…¥ä» d_model ç»´åº¦æ˜ å°„åˆ° d_ff ç»´åº¦ï¼ŒåŸæ–‡ä¸­ d_ff æ¯” d_model å¤§ï¼Œé€šå¸¸æ˜¯ 4 å€ï¼ˆä¾‹å¦‚ BERT: 768 â†’ 3072ï¼‰ï¼Œå¢åŠ ç½‘ç»œå®¹é‡ï¼Œå…è®¸æ•è·æ›´å¤æ‚å…³ç³»
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        # è¾“å…¥ x: [batch_size, seq_len, d_model]
        return self.w_2(self.dropout(F.relu(self.w_1(x))))  # æ³¨æ„ æ˜¯dropoutåå†è¿›è¡Œç¬¬äºŒæ¬¡æ˜ å°„


# clonesã€SublayerConnectionã€LayerNormæ˜¯Encoderã€Decoderä»¥åŠå…¶ä»–ä¸€äº›åœ°æ–¹å…±ç”¨çš„
def clones(module, N):
    "Produce N identical layers."
    # copy.deepcopy(module)ä¿è¯æ¯ä¸€å±‚æ‹¥æœ‰ç‹¬ç«‹çš„å‚æ•°ï¼Œè€Œä¸æ˜¯å…±äº«åŒä¸€ä¸ªå¯¹è±¡ï¼Œnn.ModuleList([...])è®©è¿™äº›å±‚å¯ä»¥æ³¨å†Œåˆ° PyTorch çš„è®¡ç®—å›¾ä¸­
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


# å­å±‚è¿æ¥ï¼ˆæ®‹å·® + å½’ä¸€åŒ– + dropoutï¼‰,æ¯ä¸€ä¸ªEncoderå±‚é‡Œæœ‰ä¸¤ä¸ªå­å±‚ï¼ˆsub-layerï¼‰å³å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚å’Œå‰é¦ˆç½‘ç»œå±‚ï¼Œè€Œåœ¨æ¯ä¸ªå­å±‚çš„å¤–é¢ï¼Œéƒ½åŒ…äº†ä¸€å±‚è¿™æ ·çš„ç»“æ„ï¼šoutput=ğ‘¥+Dropout(ğ‘†ğ‘¢ğ‘ğ‘™ğ‘ğ‘¦ğ‘’ğ‘Ÿ(LayerNom(ğ‘¥)))ï¼Œè¿™å°±æ˜¯æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ– + dropout
class SublayerConnection(nn.Module):
    """
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.
    """
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        # å¯¹è¾“å…¥å…ˆåšå½’ä¸€åŒ–ï¼ˆæ³¨æ„ï¼Œè¿™é‡Œè·ŸåŸè®ºæ–‡ç¨æœ‰åŒºåˆ«ï¼Œè®ºæ–‡ä¸­æ˜¯ å…ˆæ®‹å·®å†å½’ä¸€åŒ–ï¼›è¿™é‡Œä¸ºäº†ä»£ç ç®€æ´ï¼Œä½œè€…åè¿‡æ¥äº†ï¼‰
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        "Apply residual connection to any sublayer with the same size."
        return x + self.dropout(sublayer(self.norm(x)))
    

# å¯¹è¾“å…¥çš„æœ€åä¸€ç»´ï¼ˆå³ç‰¹å¾ç»´åº¦ï¼‰åšå½’ä¸€åŒ–ï¼Œç›®æ ‡æ˜¯å¯¹æ¯ä¸ªtokenå†…éƒ¨çš„ç‰¹å¾åšè§„èŒƒåŒ–ï¼Œä½¿å¾—æ¯ä¸ªtokenè‡ªå·±çš„ç‰¹å¾çš„å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼Œä¹‹åä¹˜ä¸Šç¼©æ”¾aå†åŠ åç§»bï¼Œä½¿å¾—æ¯ä¸ª token çš„ embedding å†…éƒ¨åˆ†å¸ƒç¨³å®š
# å‡å°‘åæ–¹å·®åç§»
class LayerNorm(nn.Module):
    "Construct a layernorm module (See citation for details)."
    def __init__(self, features, eps=1e-6):
        # features è¡¨ç¤ºè¦å½’ä¸€åŒ–çš„ç‰¹å¾ç»´åº¦å¤§å°ï¼ˆå³æœ€åä¸€ç»´çš„å¤§å°ï¼‰
        # a_2ä¹Ÿç§°weightï¼Œb_2ä¹Ÿç§°biasï¼Œéƒ½æ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œ.ones/.zerosåˆå§‹åŒ–ä¸ºå…¨1/å…¨0ï¼Œnn.Parameterå°†å…¶æ³¨å†Œä¸ºæ¨¡å‹å‚æ•°ï¼ˆä¼šè¢«ä¼˜åŒ–å™¨æ›´æ–°ï¼‰ï¼Œå½¢çŠ¶ä¸º (features,)
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps  

    def forward(self, x):
        # x æ˜¯ä¸€ä¸ªå¼ é‡ï¼Œé€šå¸¸å½¢çŠ¶ä¸º (batch_size, seq_len, features) æˆ– (batch_size, features)ï¼Œæœ€åä¸€ç»´æ˜¯ featuresï¼ˆè¦è§„èŒƒåŒ–çš„ç»´åº¦ï¼‰
        mean = x.mean(-1, keepdim=True) # å¯¹æ¯ä¸ªæ ·æœ¬åœ¨æœ€åä¸€ç»´ï¼ˆ-1ï¼‰ï¼ˆç‰¹å¾ç»´ï¼‰ä¸Šæ±‚å¹³å‡å€¼ï¼Œkeepdim=True ä¿æŒè®¡ç®—åè¯¥ç»´åº¦ä»å­˜åœ¨ï¼Œå½¢çŠ¶å˜æˆ (batch_size, seq_len, 1)ï¼ˆå¦‚æœè¾“å…¥æ˜¯ä¸‰ç»´ï¼‰ï¼Œè¿™æ ·ä¾¿äºåç»­å¹¿æ’­ï¼ˆbroadcastingï¼‰åšå‡æ³•
        std = x.std(-1, keepdim=True) # è®¡ç®—xåœ¨æœ€åä¸€ç»´çš„æ ‡å‡†å·®
        # å¹¿æ’­è¡Œä¸ºself.a_2 å’Œ self.b_2 çš„å½¢çŠ¶æ˜¯ (features,)ï¼Œä¸å½’ä¸€åŒ–ç»“æœçš„æœ€åä¸€ç»´åŒ¹é…ï¼ŒPyTorch ä¼šè‡ªåŠ¨æŠŠå®ƒä»¬å¹¿æ’­åˆ° (batch_size, seq_len, features) ç›¸ä¹˜/ç›¸åŠ 
        # æœ€ç»ˆè¾“å‡ºå’Œè¾“å…¥ x åŒå½¢çŠ¶ï¼Œä½†åœ¨æ¯ä¸ªæ ·æœ¬çš„æœ€åä¸€ç»´ä¸Šç»è¿‡æ ‡å‡†åŒ–å¹¶çº¿æ€§å˜æ¢è¿‡
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
    

class Encoder(nn.Module):
    "Core encoder is a stack of N layers"
    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        # layeræ˜¯ä¸€ä¸ªå•å±‚ç¼–ç å™¨,ç”± Multi-Head Attention + FeedForwardç»„æˆ
        self.layers = clones(layer, N)
        # self.normå¯¹æ•´ä¸ªç¼–ç å™¨è¾“å‡ºåšä¸€æ¬¡æœ€ç»ˆçš„å±‚å½’ä¸€åŒ–LayerNorm
        self.norm = LayerNorm(layer.size)
        
    def forward(self, x, mask):
        # è¾“å…¥çš„xé€šå¸¸æ˜¯è¯å‘é‡ embedding + ä½ç½®ç¼–ç 
        "Pass the input (and mask) through each layer in turn."
        # ä¾æ¬¡é€šè¿‡æ¯ä¸€å±‚ layer,æœ€ç»ˆé€šè¿‡ä¸€ä¸ª LayerNorm è¾“å‡º
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
    

class EncoderLayer(nn.Module):
    "Encoder is made up of self-attn and feed forward (defined below)"
    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn #å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆä¸€èˆ¬æ˜¯ä¸€ä¸ª MultiHeadedAttention ç±»å®ä¾‹ï¼‰
        self.feed_forward = feed_forward #å‰é¦ˆç½‘ç»œï¼ˆPositionwiseFeedForwardï¼‰
        self.sublayer = clones(SublayerConnection(size, dropout), 2) #ä¸ºæ¯ä¸ªå­å±‚åŒ…ä¸Šä¸€ä¸ª SublayerConnectionï¼ˆæ‰€ä»¥ä¸€ä¸ª layer å†…æœ‰ä¸¤ä¸ªæ®‹å·®è¿æ¥ï¼‰
        self.size = size #é€šå¸¸ä¸º 512 æˆ– 768ï¼ˆBERT ç”¨çš„æ˜¯ 768ï¼‰

    def forward(self, x, mask):

        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        return self.sublayer[1](x, self.feed_forward) # å‰é¦ˆç½‘ç»œè¾“å‡ºæ˜¯åŸºäºattentionç»“æœçš„ï¼Œè¿™ä¸ªè¾“å‡ºä¹ŸåŒ…å«å‰ä¸€å­å±‚å³è‡ªæ³¨æ„åŠ›å±‚çš„ç»“æœ


# Decoder çš„ä»»åŠ¡æ˜¯ï¼šç»™å®š â€œå‰é¢çš„è¯â€ â†’ é¢„æµ‹ â€œä¸‹ä¸€ä¸ªè¯â€
class Decoder(nn.Module):
    "Generic N layer decoder with masking."
    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)
        
    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)
    

class DecoderLayer(nn.Module):
    "Decoder is made of self-attn, src-attn, and feed forward (defined below)"
    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 3)
 
    def forward(self, x, memory, src_mask, tgt_mask):
        m = memory
        # Masked Self-Attentionï¼ˆåªèƒ½çœ‹è§è‡ªå·±ä¹‹å‰çš„è¯ï¼‰ï¼Œå’Œ Encoder çš„è‡ªæ³¨æ„åŠ›å‡ ä¹ä¸€æ ·ï¼Œä¸åŒçš„æ˜¯åŠ äº† tgt_maskï¼Œç”¨æ¥é˜»æ­¢æ¨¡å‹çœ‹åˆ°æœªæ¥çš„ token
        # ç¬¬ä¸€ä¸ªä¸ºè‡ªæ³¨æ„æœºåˆ¶Self-Attentionï¼Œç¬¬äºŒä¸ªäº¤å‰æ³¨æ„åŠ›æœºåˆ¶Encoder-Decoder Attention
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
        # Encoder-Decoder Attentionï¼ˆèƒ½çœ‹è§Encoderè¾“å‡ºï¼‰ï¼ŒDecoderçš„å…³é”®åˆ›æ–°ç‚¹ï¼Œè¯¥å±‚å¯¹ç¼–ç å™¨çš„è¾“å‡ºæ‰§è¡Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
        # è¿™è¡Œçš„ä½œç”¨æ˜¯ï¼šè®© decoder ç”¨å½“å‰çš„ç›®æ ‡åºåˆ—è¡¨ç¤ºï¼ˆä½œä¸º Queryï¼‰ï¼Œå»â€œè¯¢é—®â€ encoder çš„è¾“å‡ºï¼ˆä½œä¸º Key å’Œ Valueï¼‰ï¼Œä»è€ŒæŠŠæºå¥çš„ä¿¡æ¯èåˆè¿› decoder çš„è¡¨ç¤ºé‡Œ
        # xä¸€èˆ¬ä¸º [batch, tgt_len, d_model]ï¼Œmï¼ˆmemoryï¼‰ï¼šEncoder çš„è¾“å‡ºï¼ˆä¹Ÿç§° memoryï¼‰ï¼Œå½¢çŠ¶ä¸€èˆ¬ä¸º [batch, src_len, d_model]
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
        return self.sublayer[2](x, self.feed_forward)
    

# ç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸º (1, size, size) çš„å¸ƒå°”çŸ©é˜µï¼ŒçŸ©é˜µä¸­ä¸Šä¸‰è§’ï¼ˆæœªæ¥ä½ç½®ï¼‰è¢«æ ‡ä¸º Falseï¼ˆä¸å¯è§ï¼‰ï¼Œä¸‹ä¸‰è§’å’Œå¯¹è§’çº¿ä¸º Trueï¼ˆå¯è§ï¼‰ï¼Œç”¨äºå±è”½æ‰â€œæœªæ¥è¯â€ï¼Œä¿è¯åœ¨åš masked self-attention æ—¶ï¼Œä½ç½® i åªèƒ½çœ‹åˆ° â‰¤ i çš„ä½ç½®ï¼Œä¸èƒ½çœ‹åˆ°æœªæ¥ï¼ˆ> iï¼‰çš„ token
def subsequent_mask(size):
    "Mask out subsequent positions."
    attn_shape = (1, size, size)
    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')   # np.triu(..., k=1) ç”Ÿæˆä¸Šä¸‰è§’ï¼ˆä¸å«å¯¹è§’çº¿ï¼‰çš„ 1ï¼Œå…¶ä»–ä¸º 0,æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªtoken
    return torch.from_numpy(subsequent_mask) == 0  # == 0 æŠŠ 0 â†’ Trueï¼ˆå¯è§ï¼‰ï¼Œ1 â†’ Falseï¼ˆè¢« maskï¼‰,è¿”å›çš„æ˜¯ torch.BoolTensorï¼ˆTrue/Falseï¼‰ï¼Œæ–¹ä¾¿ä¸ attention scores ä¸€èµ·ä½¿ç”¨


class EncoderDecoder(nn.Module):
    """
    A standard Encoder-Decoder architecture. Base for this and many 
    other models.
    """
    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator
        
    def forward(self, src, tgt, src_mask, tgt_mask):
        "Take in and process masked src and target sequences."
        return self.decode(self.encode(src, src_mask), src_mask,
                            tgt, tgt_mask)
    
    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)
    
    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
    

class Generator(nn.Module):    
    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab)

    def forward(self, x):
        return F.log_softmax(self.proj(x), dim=-1)
    

# è¿™é‡Œå®šä¹‰äº†ä¸€ä¸ªæ¥å—è¶…å‚æ•°å¹¶ç”Ÿæˆå®Œæ•´æ¨¡å‹çš„å‡½æ•°
def make_model(src_vocab, tgt_vocab, N, 
               d_model=512, d_ff=2048, h=8, dropout=0.1):
    c = copy.deepcopy  # åœ¨make_modelä¸­ç”¨å®ƒå¯ä»¥äº§ç”Ÿè‹¥å¹²ä¸ªç›¸äº’ç‹¬ç«‹çš„å±‚ï¼ˆæ¯å±‚æœ‰è‡ªå·±çš„æƒé‡ï¼‰ï¼Œè€Œä¸æ˜¯è®©å¤šä¸ªå±‚å…±äº«åŒä¸€ç»„å‚æ•°
    attn = MultiHeadedAttention(h, d_model)
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)
    model = EncoderDecoder(
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        Decoder(DecoderLayer(d_model, c(attn), c(attn), 
                             c(ff), dropout), N),
        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),
        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
        Generator(d_model, tgt_vocab))

    # Initialize parameters with Glorot / fan_avg.
    for p in model.parameters():  # éå†æ¨¡å‹çš„æ‰€æœ‰å¯å­¦ä¹ å‚æ•°
        if p.dim() > 1:  # åªæœ‰ç»´åº¦å¤§äº 1 çš„å¼ é‡æ‰åšåˆå§‹åŒ–ï¼Œè¿™é€šå¸¸ç­›æ‰ä¸€ç»´çš„åç½®ï¼ˆä¾‹å¦‚biasï¼‰æˆ–æ ‡é‡å‚æ•°ï¼Œåªå¯¹æƒé‡å¼ é‡ï¼ˆçŸ©é˜µã€å·ç§¯æ ¸ã€åµŒå…¥çŸ©é˜µç­‰ï¼‰åˆå§‹åŒ–
            nn.init.xavier_uniform(p)  # å¯¹è¯¥æƒé‡å¼ é‡åš Xavierï¼ˆGlorotï¼‰å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–ï¼ˆæŠŠå‚æ•°å¡«æˆç¬¦åˆæŸä¸ªåŒºé—´çš„å‡åŒ€éšæœºæ•°ï¼‰
    return model


# Batches and Masking
# ä½œç”¨æ˜¯åˆ›å»ºå¹¶åŒ…è£…è®­ç»ƒè¦ç”¨çš„srcã€tgtä»¥åŠEncoderå’ŒDecoderåˆ†åˆ«çš„æ©ç ç­‰
# è®­ç»ƒæ ‡å‡†ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹æ‰€éœ€çš„ä¸€äº›å·¥å…·ã€‚é¦–å…ˆå®šä¹‰ä¸€ä¸ªæ‰¹å¤„ç†å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«ç”¨äºè®­ç»ƒçš„æºè¯­å¥å’Œç›®æ ‡è¯­å¥ï¼Œä»¥åŠç”¨äºæ„å»ºæ©ç çš„å¯¹è±¡
# åœ¨maskä¸­,åˆ—ï¼ˆKey æ–¹å‘ï¼‰ä¸º 0 â†’ è¿™ä¸ª token ä¸å…è®¸è¢«ä»»ä½•äººçœ‹åˆ°,è¡Œï¼ˆQuery æ–¹å‘ï¼‰ä¸º 0 â†’ è¿™ä¸ª token è‡ªå·±ä¸èƒ½çœ‹åˆ«äººï¼ˆä½†ä¸€èˆ¬ä¸è¿™ä¹ˆåšï¼‰
# src=è¾“å…¥åºåˆ—ï¼ˆç»™ Encoder çš„ï¼‰,ç”¨äºç¼–ç è¾“å…¥å†…å®¹,åªè¿›å…¥ Encoder,å½¢çŠ¶ä¸€èˆ¬ä¸º [batch, src_len]ã€‚tgt=è¾“å‡ºåºåˆ—ï¼ˆç»™ Decoder çš„ï¼‰,ç”¨äºè®­ç»ƒæ—¶â€œæ•™â€æ¨¡å‹å¦‚ä½•ç”Ÿæˆä¸‹ä¸€è¯â€,åªè¿›å…¥ Decoder,å½¢çŠ¶ä¸€èˆ¬ä¸º [batch, tgt_len]
# Encoderè¾“å…¥src,å¯ä»¥å…¨å±€çœ‹,ç†è§£å¥å­æ•´ä½“å«ä¹‰;Decoderè‡ªæ³¨æ„åŠ›è¾“å…¥trg,ä¸å…è®¸çœ‹æœªæ¥è¯,ä¿è¯è‡ªå›å½’ç”Ÿæˆ;Decoder â†’ Encoderäº¤å‰æ³¨æ„åŠ›è¾“å…¥trg â†’ src,å¯ä»¥å…¨å±€çœ‹æºå¥,è¾“å‡ºåº”å‚è€ƒè¾“å…¥å†…å®¹
class Batch:
    "Object for holding a batch of data with mask during training."
    def __init__(self, src, tgt=None, pad=0):
        self.src = src
        # ç”Ÿæˆç”¨äº encoder è‡ªæ³¨æ„åŠ›é‡Œå±è”½ padding çš„ mask     
        self.src_mask = (src != pad).unsqueeze(-2)  #  # pad æ˜¯ padding çš„ idï¼ˆä¾‹å¦‚ 0ï¼‰,src != pad ä¼šç”Ÿæˆä¸€ä¸ªå¸ƒå°”å¼ é‡ï¼Œå½¢çŠ¶ä»ç„¶æ˜¯ (batch, src_len)
        if tgt is not None:
            # è®­ç»ƒç”¨çš„å…¸å‹åšæ³•æ˜¯æŠŠç›®æ ‡åºåˆ—é”™å¼€ä¸€ä½,decoder çš„è¾“å…¥æ˜¯ç›®æ ‡åºåˆ—çš„å·¦ç§»ç‰ˆæœ¬ï¼ˆä¸åŒ…å«æœ€åä¸€ä¸ª tokenï¼‰ï¼Œdecoder çš„è®­ç»ƒç›®æ ‡æ˜¯ç›®æ ‡åºåˆ—çš„å³ç§»ç‰ˆæœ¬ï¼ˆä¸åŒ…å«ç¬¬ä¸€ä¸ª tokenï¼‰
            # self.trg ä¸ self.trg_y æ˜¯ä¸€å¯¹ parallel sequencesï¼šè®­ç»ƒæ—¶æ¨¡å‹ç”¨ self.trg é¢„æµ‹ self.trg_yï¼ˆæ¯æ¬¡å‘å³ä¸€æ­¥ï¼‰ã€‚trgæ˜¯Decoderè¾“å…¥ï¼Œtrg_yæ˜¯Decoderç›®æ ‡è¾“å‡º
            self.trg = tgt[:, :-1]
            self.trg_y = tgt[:, 1:] 
            # è°ƒç”¨é™æ€æ–¹æ³•ç”Ÿæˆdecoderçš„subsequent maskï¼Œè¿™ä¸ªmaskåŒæ—¶è¦å±è”½paddingï¼ˆtrg ä¸­ <pad> çš„ä½ç½®ï¼‰ä»¥åŠfuture positionsï¼ˆé˜²æ­¢ decoder åœ¨è®­ç»ƒæ—¶çœ‹åˆ°â€œæœªæ¥çš„è¯â€ï¼‰
            self.trg_mask = self.make_std_mask(self.trg, pad)
            # ç»Ÿè®¡è¿™ä¸ª batch ä¸­æœ‰æ•ˆç›®æ ‡ token çš„æ€»æ•°ï¼ˆä¸åŒ…æ‹¬ padï¼‰ï¼Œç”¨äºåœ¨è®¡ç®—å¹³å‡ loss æ—¶é™¤ä»¥æœ‰æ•ˆ token æ•°
            self.ntokens = (self.trg_y != pad).data.sum()  # è®°ä½ä¸ºä»€ä¹ˆè¦ç”¨self.trg_yè€Œä¸ç”¨self.trg,æ¨¡å‹è¦å­¦ä¼šä»€ä¹ˆæ—¶å€™è®©å¥å­ç»“æŸ
    
    @staticmethod
    def make_std_mask(trg, pad):
        "Create a mask to hide padding and future words."
        tgt_mask = (trg != pad).unsqueeze(-2)                        
        tgt_mask = tgt_mask & Variable(
            subsequent_mask(trg.size(-1)).type_as(tgt_mask.data))
        return tgt_mask


# æ¨¡å‹è®­ç»ƒå‡½æ•°
# æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªé€šç”¨çš„è®­ç»ƒå’Œè¯„åˆ†å‡½æ•°æ¥è·Ÿè¸ªæŸå¤±ã€‚æˆ‘ä»¬ä¼ å…¥ä¸€ä¸ªé€šç”¨çš„æŸå¤±è®¡ç®—å‡½æ•°ï¼Œè¯¥å‡½æ•°ä¹Ÿè´Ÿè´£å¤„ç†å‚æ•°æ›´æ–°
# Training Loopè®­ç»ƒå¾ªç¯ã€‚è¾“å…¥å¤šä¸ª batchï¼ˆdata_iterï¼‰,å¯¹æ¯ä¸ª batch åšä¸€æ¬¡ forward + loss + backward + æ›´æ–°å‚æ•°,åŒæ—¶è®°å½•æŸå¤±å’Œè®­ç»ƒæ•ˆç‡ï¼ˆååé‡ Tokens/secï¼‰
def run_epoch(data_iter, model, loss_compute):  # data_itersæ˜¯ä¸€ä¸ªèƒ½ä¸æ–­æä¾› Batchï¼ˆæ‰¹æ•°æ®ï¼‰çš„è¿­ä»£å™¨,Batchå¯¹è±¡
    "Standard Training and Logging Function"
    start = time.time()
    total_tokens = 0
    total_loss = 0
    tokens = 0  # tokensï¼šç”¨äºæ¯éš”ä¸€æ®µæ—¶é—´åŠ¨æ€è®¡ç®—ååç‡ï¼ˆTokens per Secï¼‰
    for i, batch in enumerate(data_iter):
        out = model.forward(batch.src, batch.trg,  # batch.src â†’ Encoder è¾“å…¥,batch.trg â†’ Decoder è¾“å…¥ï¼ˆé”™ä½åçš„åºåˆ—ï¼Œç”¨äºæ•™æ¨¡å‹é¢„æµ‹ä¸‹ä¸€è¯ï¼‰
                            batch.src_mask, batch.trg_mask)  # model.forward è¾“å‡ºï¼šout: [batch, tgt_len, vocab_size],è¡¨ç¤ºæ¨¡å‹å¯¹æ¯ä¸ªä½ç½®è¾“å‡ºçš„è¯é¢„æµ‹åˆ†å¸ƒ
        loss = loss_compute(out, batch.trg_y, batch.ntokens)  # loss_compute å†…éƒ¨loss = criterion(out, trg_y)
                                                                                #loss.backward()
                                                                                #optimizer.step()
                                                                                #optimizer.zero_grad()
        total_loss += loss
        total_tokens += batch.ntokens  # batch.ntokens æ˜¯æœ¬ batch ä¸­ éœ€è¦é¢„æµ‹çš„ token æ•°ï¼ˆä¸åŒ…æ‹¬ padï¼‰
        tokens += batch.ntokens
        if i % 50 == 1:  # æ¯ 50 ä¸ª batch æ‰“å°ä¸€æ¬¡è®­ç»ƒé€Ÿåº¦
            elapsed = time.time() - start
            print("Epoch Step: %d Loss: %f Tokens per Sec: %f" %
                    (i, loss / batch.ntokens, tokens / elapsed))  # loss / batch.ntokensï¼šæ¯ä¸ª token çš„å¹³å‡æŸå¤±ï¼ˆæ›´ç¨³å®šï¼‰;tokens / elapsedæ˜¯è®­ç»ƒé€Ÿåº¦, è¶Šå¤§è¯´æ˜è®­ç»ƒè¶Šå¿«
            start = time.time()
            tokens = 0
    return total_loss / total_tokens  # è¿”å›æ•´ä¸ª epoch çš„å¹³å‡æŸå¤±


# è®­ç»ƒæ•°æ®å’Œæ‰¹å¤„ç†ï¼Œå®ç°éå›ºå®š batch_sizeåˆ†æ‰¹,å› ä¸ºæ ·æœ¬é•¿åº¦æœ‰æ—¶å·®å¼‚è¾ƒå¤§
# æˆ‘ä»¬ä½¿ç”¨åŒ…å«çº¦ 450 ä¸‡ä¸ªå¥å­å¯¹çš„æ ‡å‡† WMT 2014 è‹±å¾·æ•°æ®é›†è¿›è¡Œè®­ç»ƒ
# å¥å­é‡‡ç”¨å­—èŠ‚å¯¹ç¼–ç ï¼Œå…¶æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€å…±äº«è¯æ±‡è¡¨çº¦ 37000 ä¸ªè¯å…ƒã€‚å¯¹äºè‹±æ³•æ•°æ®é›†ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†è§„æ¨¡æ›´å¤§çš„ WMT 2014 è‹±æ³•æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å« 3600 ä¸‡ä¸ªå¥å­ï¼Œå¹¶å°†è¯å…ƒæ‹†åˆ†ä¸º 32000 ä¸ªè¯æ®µçš„è¯æ±‡è¡¨
# å¥å­å¯¹æŒ‰å¤§è‡´åºåˆ—é•¿åº¦è¿›è¡Œåˆ†ç»„ã€‚æ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡åŒ…å«ä¸€ç»„å¥å­å¯¹ï¼Œå…¶ä¸­å¤§çº¦åŒ…å« 25000 ä¸ªæºè¯å…ƒå’Œ 25000 ä¸ªç›®æ ‡è¯å…ƒ
# æˆ‘ä»¬å°†ä½¿ç”¨ torchtext è¿›è¡Œæ‰¹å¤„ç†ã€‚ä¸‹æ–‡å°†è¯¦ç»†è®¨è®ºè¿™ä¸€ç‚¹ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬åœ¨ torchtext å‡½æ•°ä¸­åˆ›å»ºæ‰¹æ¬¡ï¼Œä»¥ç¡®ä¿å¡«å……åˆ°æœ€å¤§æ‰¹æ¬¡å¤§å°åçš„æ‰¹æ¬¡å¤§å°ä¸è¶…è¿‡é˜ˆå€¼ï¼ˆå¦‚æœæˆ‘ä»¬æœ‰ 8 ä¸ª GPUï¼Œåˆ™ä¸º 25000ï¼‰
global max_src_in_batch, max_tgt_in_batch
def batch_size_fn(new, count, sofar):
    "Keep augmenting batch and calculate total number of tokens + padding."
    global max_src_in_batch, max_tgt_in_batch
    if count == 1:
        max_src_in_batch = 0
        max_tgt_in_batch = 0
    max_src_in_batch = max(max_src_in_batch,  len(new.src))
    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)  # è¿™é‡Œç”¨äº† + 2ï¼šé€šå¸¸æ˜¯å› ä¸ºç›®æ ‡åºåˆ—åœ¨é€å…¥ decoder æ—¶ä¼šåŠ ä¸Š <sos>ï¼ˆæˆ– <bos>ï¼‰å’Œ <eos>
    # å®é™… decoder input/label çš„é•¿åº¦ä¼šæ¯”åŸå§‹ new.trg é•¿ 2ï¼ˆæˆ–ä½œè€…ä¸ºäº†ä¿å®ˆä¼°è®¡è€ŒåŠ  2ï¼‰ã€‚æ‰€ä»¥è¿™é‡ŒæŠŠç›®æ ‡é•¿åº¦é¢„ç•™äº†å¼€å§‹/ç»“æŸä¸¤ä¸ªä½ç½®
    src_elements = count * max_src_in_batch  # å¦‚æœæŠŠå½“å‰ batch ä¸­ count ä¸ªæ ·æœ¬éƒ½ padding åˆ° max_src_in_batchï¼Œé‚£ä¹ˆæºä¾§æ€» token æ•°
    tgt_elements = count * max_tgt_in_batch
    return max(src_elements, tgt_elements)  # è¿”å›æºä¾§å’Œç›®æ ‡ä¾§ä¸¤ç§ä¼°ç®—ä¸­æ›´å¤§çš„é‚£ä¸ª,ä½œä¸ºâ€œè¿™ä¸ª candidate batch çš„å½“å‰ä»£ä»·ï¼ˆtoken æ•°ï¼‰â€
                                            # æ•°æ®åŠ è½½å™¨ä¼šæ ¹æ®è¿™ä¸ªè¿”å›å€¼æ¥å†³å®šæ˜¯å¦å†åŠ å…¥æ–°æ ·æœ¬ï¼ˆä¾‹å¦‚ä¿æŒè¿”å›å€¼ <= æŸä¸ª token ä¸Šé™ï¼‰ï¼Œä»è€Œå®ç°æŒ‰ token æ•°åŠ¨æ€å®š batch


# Transformerè®ºæ–‡é‡Œçš„å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆNoamLRï¼‰
# è®¾è®¡åŸå› æ˜¯è®­ç»ƒåˆæœŸç›´æ¥ç”¨å¾ˆå¤§çš„lrä¼šç ´åå‚æ•°ï¼ˆå°¤å…¶æ˜¯ Transformer é‡Œå¤šå±‚æ®‹å·®ä¸ LayerNorm çš„é…åˆå¾ˆæ•æ„Ÿï¼‰ï¼Œwarmup é˜¶æ®µè®©æ¨¡å‹â€œæ…¢æ…¢çƒ­èº«â€åˆ°ä¸€ä¸ªåˆé€‚çš„å­¦ä¹ ç‡ï¼›ä¹‹åæŒ‰1/æ ¹å·stepè¡°å‡å¯ä»¥ä¿è¯æ”¶æ•›ç¨³å®š
# æˆ‘ä»¬åœ¨ä¸€å°é…å¤‡8ä¸ªNVIDIA P100 GPUçš„æœºå™¨ä¸Šè®­ç»ƒæ¨¡å‹,å¯¹äºæœ¬æ–‡æ‰€è¿°çš„è¶…å‚æ•°åŸºç¡€æ¨¡å‹ï¼Œæ¯ä¸ªè®­ç»ƒæ­¥éª¤è€—æ—¶çº¦0.4ç§’ã€‚åŸºç¡€æ¨¡å‹æ€»å…±è®­ç»ƒäº†10ä¸‡æ­¥ï¼Œè€—æ—¶12å°æ—¶ã€‚å¯¹äºå¤§å‹æ¨¡å‹ï¼Œæ¯æ­¥è€—æ—¶1.0 ç§’ã€‚å¤§å‹æ¨¡å‹è®­ç»ƒäº†30ä¸‡æ­¥ï¼ˆ3.5 å¤©ï¼‰
# NoamOptæ˜¯ä¸€ä¸ªåŒ…è£…å™¨ï¼ˆwrapperï¼‰,æŠŠä¸€ä¸ªæ ‡å‡†çš„PyTorchä¼˜åŒ–å™¨ï¼ˆæ¯”å¦‚ AdamåŒ…èµ·æ¥,æ¯æ¬¡æ›´æ–°å‚æ•°å‰å…ˆæŒ‰è®ºæ–‡çš„Noamå­¦ä¹ ç‡ç­–ç•¥è®¡ç®—å¹¶è®¾ç½®å½“å‰å­¦ä¹ ç‡,å†è°ƒç”¨ä¼˜åŒ–å™¨çš„step(),get_std_opt æ˜¯ç”¨è®ºæ–‡æ¨èçš„è¶…å‚åˆ›å»ºä¸€ä¸ªå°è£…å™¨
class NoamOpt:
    "Optim wrapper that implements rate."
    def __init__(self, model_size, factor, warmup, optimizer):
        self.optimizer = optimizer  # è¢«å°è£…çš„å®é™…ä¼˜åŒ–å™¨ï¼ˆå¦‚ torch.optim.Adam(...)ï¼‰
        self._step = 0  # å†…éƒ¨è®¡æ•°
        self.warmup = warmup  
        self.factor = factor  # ç¼©æ”¾å› å­ï¼ˆè®ºæ–‡é‡Œå¸¸ç”¨1æˆ–2ï¼Œä»£ç ç”¨2ï¼‰
        self.model_size = model_size  # Transformerçš„éšè—ç»´åº¦dmodelï¼ˆä¾‹å¦‚512ï¼‰
        self._rate = 0  # è®°å½•å½“å‰ lr
        # æ³¨æ„optimizer.zero_grad()ä»éœ€åœ¨å¤–éƒ¨æˆ–loss_computeé‡Œè°ƒç”¨ï¼ˆNoamOptä¸åšzero_gradï¼‰
    
    def step(self):
        "Update parameters and rate"
        self._step += 1
        rate = self.rate()
        for p in self.optimizer.param_groups:
            p['lr'] = rate  # æŠŠè¿™ä¸ªrateå†™å…¥ä¼˜åŒ–å™¨çš„æ¯ä¸ªparam_groupçš„ 'lr'
        self._rate = rate
        self.optimizer.step()  # è°ƒç”¨åº•å±‚ä¼˜åŒ–å™¨self.optimizer.step()æ›´æ–°å‚æ•°
        
    def rate(self, step = None):  # è¿™å°±æ˜¯NoamLRçš„æ•°å­¦å…¬å¼ï¼Œç­‰ä»·äºè®ºæ–‡ä¸­ã€‚å…ˆçº¿æ€§å¢é•¿ï¼ˆwarmup é˜¶æ®µï¼‰ï¼Œè¾¾åˆ°å³°å€¼åæŒ‰1/æ ¹å·stepè¡°å‡
        if step is None:
            step = self._step
        return self.factor * \
            (self.model_size ** (-0.5) *
            min(step ** (-0.5), step * self.warmup ** (-1.5)))
        
def get_std_opt(model):  # åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªå°è£…äº†Adam çš„é…ç½®å¥½å‚æ•°çš„NoamOptä¼˜åŒ–å™¨
    # d_modelä»model.src_embed[0].d_modelå–å¾—ï¼ˆé€šå¸¸src_embedæ˜¯nn.Sequentialæˆ–tupleï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æœ‰d_modelå±æ€§ï¼‰
    # factor=2ã€warmup=4000ï¼ˆè®ºæ–‡æ¨èï¼‰ï¼Œå¹¶ç”¨ Adam(..., lr=0, betas=(0.9,0.98), eps=1e-9)ä½œä¸ºå†…éƒ¨ä¼˜åŒ–å™¨ã€‚lr=0æ˜¯å› ä¸ºå®é™…lrç”±NoamOptåŠ¨æ€è®¾ç½®
    return NoamOpt(model.src_embed[0].d_model, 2, 4000,
            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)) 


# æ„é€ æŸå¤±å‡½æ•°
# åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†æ ‡ç­¾å¹³æ»‘æ³•æ¥è¯„ä¼°å€¼,è¿™ä¼šé™ä½å›°æƒ‘åº¦,å› ä¸ºæ¨¡å‹ä¼šå­¦ä¹ å˜å¾—æ›´åŠ ä¸ç¡®å®š,ä½†ä¼šæé«˜å‡†ç¡®ç‡å’Œ BLEU åˆ†æ•°
# æ ‡ç­¾å¹³æ»‘æŠŠç¡¬one-hotæ ‡ç­¾å˜æˆä¸€ä¸ªâ€œç¨å¾®è¢«å¹³æ»‘è¿‡â€çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆæŠŠä¸€å°éƒ¨åˆ†è´¨é‡ä»æ­£ç¡®ç±»åˆ«åˆ†ç»™å…¶å®ƒç±»åˆ«ï¼‰ï¼Œä»¥é™ä½æ¨¡å‹è¿‡åº¦è‡ªä¿¡ã€æ”¹å–„æ³›åŒ–ä¸ç¨³å®šè®­ç»ƒã€‚è¿™é‡Œç”¨çš„æ˜¯KLæ•£åº¦æŠŠæ¨¡å‹è¾“å‡ºä¸è¿™ä¸ªå¹³æ»‘åçš„ç›®æ ‡åˆ†å¸ƒåšåŒ¹é…
# æˆ‘ä»¬ä½¿ç”¨KL divæŸå¤±å‡½æ•°æ¥å®ç°æ ‡ç­¾å¹³æ»‘ã€‚æˆ‘ä»¬ä¸ä½¿ç”¨ç‹¬çƒ­ç¼–ç çš„ç›®æ ‡åˆ†å¸ƒï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªåŒ…å«confidenceæ­£ç¡®å•è¯å’Œå…¶ä½™å•è¯smoothingåˆ†å¸ƒåœ¨æ•´ä¸ªè¯æ±‡è¡¨ä¸­çš„åˆ†å¸ƒ
class LabelSmoothing(nn.Module):
    "Implement label smoothing."
    def __init__(self, size, padding_idx, smoothing=0.0):  
        super(LabelSmoothing, self).__init__()
        # nn.KLDivLoss è¦æ±‚ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ log-probabilitiesï¼ˆlog Pï¼‰,ç¬¬äºŒä¸ªå‚æ•°æ˜¯æ¦‚ç‡åˆ†å¸ƒ(Q),å³è®¡ç®—âˆ‘ğ‘„log(ğ‘„/ğ‘ƒ)
        self.criterion = nn.KLDivLoss(size_average=False)  # åˆ›å»ºKLæ•£åº¦æŸå¤±å‡½æ•°,size_average=Falseæ„å‘³ç€è¿”å›æ€»å’Œï¼ˆsumï¼‰ï¼Œç°ä»£PyTorchç”¨reduction='sum'
        self.padding_idx = padding_idx  # è¡¨ç¤º padding å¯¹åº”çš„ç±»åˆ«ç´¢å¼•ï¼ˆé€šå¸¸ 0ï¼‰ï¼Œéœ€è¦åœ¨ç›®æ ‡åˆ†å¸ƒé‡ŒæŠŠå®ƒå¤„ç†ä¸º 0ï¼ˆä¸åˆ†é…æ¦‚ç‡ï¼‰
        self.smoothing = smoothing  # å¹³æ»‘å¼ºåº¦sï¼ˆä¾‹å¦‚ 0.1ï¼‰
        self.confidence = 1.0 - smoothing  # æ­£ç¡®ç±»åˆ«è¢«åˆ†é…åˆ°çš„æ¦‚ç‡
        self.size = size  # sizeï¼šè¾“å‡ºç±»åˆ«æ•°V
        self.true_dist = None  # åœ¨ forward è¿‡ç¨‹ä¸­ä¿å­˜â€œæ„é€ åçš„æ ‡ç­¾å¹³æ»‘åçš„çœŸå®åˆ†å¸ƒï¼ˆtarget distributionï¼‰â€ï¼Œè®©ä½ å¯ä»¥åœ¨è®­ç»ƒåæŸ¥çœ‹å®ƒã€å¯è§†åŒ–å®ƒã€debug ç”¨

    def forward(self, x, target):  # target å°±æ˜¯è®­ç»ƒä¸­æ¨¡å‹æœŸæœ›è¾“å‡ºçš„æ­£ç¡®ç­”æ¡ˆ
        # xæ˜¯æ¨¡å‹è¾“å‡ºçš„logitsï¼Œå½¢çŠ¶[batch_size, V]ï¼ˆé€šå¸¸å…ˆåš x = log_softmax(model_out) å†ä¼ å…¥ï¼‰ï¼›targetï¼šæ•´å‹æ ‡ç­¾ï¼Œå½¢çŠ¶ [N]ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ 0..V-1ã€‚ï¼ˆè‹¥æ˜¯åºåˆ—ä»»åŠ¡é€šå¸¸æŠŠ batch å±•å¹³æˆ Nï¼‰
        assert x.size(1) == self.size  # æ£€æŸ¥ç±»åˆ«ç»´åº¦ä¸€è‡´
        # å…ˆç”¨ x.data.clone()ï¼ˆæ—§å†™æ³•ï¼Œå¾—åˆ°ä¸€ä¸ª tensor åŒ shapeï¼‰åˆ›å»ºä¸€ä¸ªå¼ é‡å ä½ï¼Œç”¨æ¥æ„é€ ç›®æ ‡åˆ†å¸ƒã€‚æ³¨æ„ï¼šx.data å±äºè€ APIï¼Œç°ä»£è¯·ç”¨ x.detach().clone() æˆ–åœ¨ torch.no_grad() ä¸‹æ“ä½œ
        true_dist = x.data.clone()
        # fill_ æŠŠæ¯ä¸ªä½ç½®å…ˆå¡«å…¥å¹³æ»‘åˆ†é…å€¼ï¼Œè¿™é‡Œç”¨äº† self.smoothing / (self.size - 2)
        # ç”¨-2å› ä¸ºåœ¨åç»­ä¼šæŠŠ padding_idx çš„åˆ—è®¾ä¸º0å¹¶æŠŠæ­£ç¡®ç±»åˆ«åˆ—è®¾ç½®ä¸ºconfidenceï¼Œæ‰€ä»¥å‡åŒ€åˆ†é…çš„åˆ†æ¯è¦å‡å» 2ï¼ˆä¸€ä¸ªæ˜¯æ­£ç¡®ç±»åˆ«ï¼Œä¸€ä¸ªæ˜¯ padding ç±»ï¼‰â€”ä¹Ÿå°±æ˜¯è¯´smoothingçš„è´¨é‡å‡åŒ€åˆ†é…åˆ°é™¤æ­£ç¡®ç±»åˆ«å’Œpaddingä¹‹å¤–çš„å…¶ä»–ç±»åˆ«ä¸Š
        true_dist.fill_(self.smoothing / (self.size - 2))
        # scatter_(1, index, value)ï¼šåœ¨ dim=1ï¼ˆç±»åˆ«ç»´ï¼‰æŒ‰ç…§ target ç´¢å¼•æŠŠ value å†™å…¥å¯¹åº”ä½ç½®ï¼›ä½œç”¨ï¼šæŠŠæ¯ä¸ªæ ·æœ¬çš„æ­£ç¡®ç±»åˆ«ä½ç½®èµ‹ä¸º confidenceï¼ˆå³ 1 - smoothingï¼‰
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0  # æŠŠ padding ç±»çš„æ¦‚ç‡å¼ºåˆ¶ä¸º 0ï¼Œä¿è¯ padding ä¸è¢«å½“æˆç›®æ ‡åˆ†é…æ¦‚ç‡
        # mask æ‰¾åˆ°é‚£äº›æ ·æœ¬å…¶ç›®æ ‡æ ‡ç­¾æœ¬èº«å°±æ˜¯ paddingï¼ˆä¾‹å¦‚åœ¨ seq-to-seq ä¸­ï¼ŒæŸäº›ä½ç½®æ˜¯ padï¼Œå¹¶ä¸å‚ä¸é¢„æµ‹ï¼‰
        # å¦‚æœå­˜åœ¨è¿™æ ·çš„æ ·æœ¬ï¼Œindex_fill_ æŠŠå¯¹åº”æ ·æœ¬æ•´è¡Œï¼ˆæ•´æ¡æ ·æœ¬çš„ç›®æ ‡åˆ†å¸ƒï¼‰ç½®ä¸º 0ï¼Œè¿™æ ·è¿™äº›ä½ç½®åœ¨è®¡ç®— KL æ—¶å¯¹æŸå¤±æ²¡æœ‰è´¡çŒ®ï¼ˆç­‰åŒäºè·³è¿‡è¿™äº›ä½ç½®ï¼‰
        # ä¹Ÿå°±æ˜¯ä¸º trg_y == pad çš„ä½ç½®æŠŠç›®æ ‡åˆ†å¸ƒå…¨éƒ¨ç½® 0ï¼ˆä¸ä¼šå‚ä¸ lossï¼‰ï¼Œå¯¹é½ ntokens çš„åšæ³•
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        # æŠŠæ„é€ å¥½çš„ç›®æ ‡åˆ†å¸ƒä¿å­˜åˆ°å®ä¾‹ä»¥ä¾¿è°ƒè¯•/æ£€æŸ¥ï¼ˆself.true_distï¼‰,ç”¨ self.criterionï¼ˆKLDivLossï¼‰è®¡ç®—æŸå¤±ï¼šç¬¬ä¸€ä¸ªå‚æ•° x åº”è¯¥æ˜¯ log Pï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ Qï¼ˆè¿™é‡Œä¼  Variable(true_dist, requires_grad=False)ï¼Œä¹Ÿå±äºè€å†™æ³•ï¼‰
        # æ³¨æ„ï¼šç°ä»£ PyTorch ä¸ç”¨ Variableï¼Œç›´æ¥ä¼  true_distï¼ˆç¡®ä¿ requires_grad=False æˆ–è€…åœ¨ torch.no_grad()ä¸‹æ„é€ å³å¯ï¼‰,è¿”å›çš„æ˜¯KLæ•£åº¦çš„å’Œï¼ˆå› ä¸º size_average=Falseï¼‰ï¼Œé€šå¸¸è®­ç»ƒä»£ç ä¼šæŒ‰ntokensåšå¹³å‡loss = loss_compute(...)/ntokens
        self.true_dist = true_dist
        return self.criterion(x, Variable(true_dist, requires_grad=False))
   

# ç¬¬ä¸€ä¸ªä¾‹å­:å…ˆå°è¯•ä¸€ä¸ªç®€å•çš„å¤åˆ¶ä»»åŠ¡ã€‚è®­ç»ƒä¸€ä¸ªå°å‹ Transformerï¼Œè®©å®ƒå­¦ä¼šæŠŠè¾“å…¥åºåˆ—åŸæ ·å¤åˆ¶åˆ°è¾“å‡ºï¼ˆå³è¾“å…¥==è¾“å‡ºï¼‰ã€‚ç»™å®šä¸€ç»„æ¥è‡ªâ€‹â€‹å°å‹è¯æ±‡è¡¨çš„éšæœºè¾“å…¥ç¬¦å·ï¼Œç›®æ ‡æ˜¯ç”Ÿæˆä¸è¾“å…¥ç¬¦å·ç›¸åŒçš„åºåˆ—
# åˆæˆæ•°æ®
def data_gen(V, batch, nbatches):
    "Generate random data for a src-tgt copy task."
    for i in range(nbatches):
        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))  # æ¯ä¸ªæ ·æœ¬é•¿åº¦å›ºå®šä¸º 10ï¼Œè¯ id åœ¨ [1, V-1]ï¼ˆ0 é€šå¸¸ä¿ç•™ä½œ <pad>ï¼‰ï¼Œå¹¶æŠŠç¬¬ä¸€ä¸ª token å¼ºåˆ¶è®¾ä¸º 1ï¼ˆå¯èƒ½ä»£è¡¨ <sos>/ç‰¹æ®Šèµ·å§‹ç¬¦ï¼‰
        data[:, 0] = 1
        src = Variable(data, requires_grad=False)
        tgt = Variable(data, requires_grad=False)
        yield Batch(src, tgt, 0)  # data æ˜¯ (batch, 10)ï¼Œå› æ­¤ src å’Œ tgt éƒ½æ˜¯ (batch, 10)
# æŸå¤±è®¡ç®—
# SimpleLossCompute æŠŠæ¨¡å‹ decoder çš„åŸå§‹è¾“å‡ºæ˜ å°„åˆ°è¯è¡¨æ¦‚ç‡ï¼Œè®¡ç®—æŒ‰ token å½’ä¸€åŒ–çš„æŸå¤±ï¼ˆå¯¹ padding åšå¿½ç•¥ï¼‰ï¼Œåå‘ä¼ æ’­å¹¶è°ƒç”¨ä¼˜åŒ–å™¨ä¸€æ­¥ï¼Œæœ€åè¿”å›æ€» lossï¼ˆä¸æ˜¯å¹³å‡å€¼ï¼‰
class SimpleLossCompute:
    "A simple loss compute and train function."
    def __init__(self, generator, criterion, opt=None):
        self.generator = generator
        self.criterion = criterion
        self.opt = opt
        
    def __call__(self, x, y, norm):  # xä¸ºæ¨¡å‹çš„åŸå§‹ decoder è¾“å‡º
        x = self.generator(x)
        # criterionè¿”å›çš„æ˜¯è¿™ä¸ª batchï¼ˆæˆ– N ä¸ª tokenï¼‰çš„æ€»æŸå¤±ï¼Œnormæ˜¯è¿™ä¸ª batch ä¸­çœŸå® token çš„æ•°é‡ï¼Œä¸ç®— padï¼Œloss / norm å°±æ˜¯æ¯ä¸ª token çš„å¹³å‡æŸå¤±
        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),  # æŠŠ (batch, tgt_len, vocab) å±•å¹³æˆ (N, vocab)ï¼ŒæŠŠç›®æ ‡ y ä» (batch, tgt_len) å±•å¹³æˆ (N,)ï¼Œå…¶ä¸­ N = batch * tgt_len
                              y.contiguous().view(-1)) / norm
        loss.backward()
        if self.opt is not None:  
            self.opt.step()  # self.opt.step()ï¼šè°ƒç”¨å¤–é¢ä¼ å…¥çš„ä¼˜åŒ–å™¨åŒ…è£…ï¼ˆå¦‚ NoamOpt.step()ï¼‰
            self.opt.optimizer.zero_grad()  # æŠŠæ‰€æœ‰å‚æ•°çš„ .grad æ¸…é›¶ï¼Œä¸ºä¸‹ä¸€æ¬¡è¿­ä»£å‡†å¤‡
        return loss.data[0] * norm  # è¿”å›æ€»çš„æŸå¤±å’Œï¼ˆä¸æœ€å¼€å§‹çš„ self.criterion(... ) çš„å’Œä¸€è‡´ï¼‰ï¼Œå› ä¸ºå‰é¢ loss è¢« / norm å¹³å‡äº†ï¼Œå› æ­¤ä¹˜å› * norm æ¢å¤ä¸ºæ€» loss ç”¨äºç»Ÿè®¡
# Greedy Decodingè´ªå©ªè§£ç 
V = 11
criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)
model = make_model(V, V, N=2)
model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,
        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))

# é‡å¤å¤šæ¬¡ï¼ˆepochï¼‰æŠŠè®­ç»ƒæ•°æ®å–‚ç»™ Transformer åšå‰å‘+åå‘ä¼ æ’­å¹¶ç”¨ Noam å­¦ä¹ ç‡è°ƒåº¦å™¨æ›´æ–°å‚æ•°ï¼›åœ¨æ¯ä¸ªrun_epochååˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼åœ¨å°æ‰¹éªŒè¯é›†ä¸Šæµ‹ä¸€æ¬¡ loss ï¼Œä»¥ç›‘æ§æ¨¡å‹æ˜¯å¦å­¦ä¼šäº†å¤åˆ¶ä»»åŠ¡
for epoch in range(10):
    model.train()
    run_epoch(data_gen(V, 30, 20), model, 
              SimpleLossCompute(model.generator, criterion, model_opt))
    model.eval()
    print(run_epoch(data_gen(V, 30, 5), model, 
                    SimpleLossCompute(model.generator, criterion, None)))

# ä¸ºäº†ç®€å•èµ·è§ï¼Œè¿™æ®µä»£ç ä½¿ç”¨è´ªå©ªè§£ç æ¥é¢„æµ‹ç¿»è¯‘ç»“æœ
def greedy_decode(model, src, src_mask, max_len, start_symbol):
    memory = model.encode(src, src_mask)  # æŠŠè¾“å…¥å¥å­ src ä¸¢è¿› Encoderï¼Œå¾—åˆ°å®ƒçš„è¯­ä¹‰è¡¨ç¤º memoryã€‚ç±»æ¯”è¯»å®Œä¸€å¥è¯åâ€œç†è§£äº†å®ƒçš„å«ä¹‰â€
    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)  # åˆå§‹åŒ–è¾“å‡ºï¼Œåªæ”¾ä¸€ä¸ª <sos>ï¼ˆå¼€å§‹æ ‡è®°ï¼‰ï¼Œå‘Šè¯‰æ¨¡å‹ï¼šâ€œå¥½ï¼Œç°åœ¨å¼€å§‹è¯´è¯ã€‚â€
    for i in range(max_len-1):  # ä¸€å£æ°”ç”Ÿæˆ max_len ä¸ªè¯
        out = model.decode(memory, src_mask,   # æŠŠå·²ç»ç”Ÿæˆçš„éƒ¨åˆ† ys é€è¿› Decoder,ç„¶åDecoderä¼šæ ¹æ®è¾“å…¥å¥å­çš„å«ä¹‰memoryã€å·²ç”Ÿæˆçš„å†…å®¹ysã€ä¸¥æ ¼çš„å› æœ Maskä¸èƒ½å·çœ‹æœªæ¥è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„å‘é‡è¡¨ç¤º
                           Variable(ys), 
                           Variable(subsequent_mask(ys.size(1))
                                    .type_as(src.data)))
        prob = model.generator(out[:, -1])  # å–æœ€åä¸€ä¸ªè¯çš„ä½ç½® â†’ æ˜ å°„åˆ°è¯è¡¨æ¦‚ç‡
        _, next_word = torch.max(prob, dim = 1)  # è´ªå©ªç­–ç•¥ï¼šé€‰æ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªè¯ã€‚ä¸è€ƒè™‘åæœï¼Œä¸åšæƒè¡¡ï¼Œå°±é€‰çœ¼å‰æœ€ä¼˜ï¼Œæ‰€ä»¥å« greedy
        next_word = next_word.data[0]  # æŠŠé€‰å‡ºæ¥çš„è¯åŠ åˆ°å¥å­æœ«å°¾ï¼Œç”¨äºä¸‹ä¸€æ­¥ç»§ç»­è¾“å…¥ï¼Œç»§ç»­é¢„æµ‹
        ys = torch.cat([ys, 
                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)
    return ys  # æœ€ç»ˆys å°±æ˜¯æ¨¡å‹ç”Ÿæˆçš„å¥å­

model.eval()
src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )
src_mask = Variable(torch.ones(1, 1, 10) )
print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))



